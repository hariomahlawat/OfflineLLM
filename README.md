# OfflineLLM

**Offlineâ€‘ready localâ€‘LAN Largeâ€‘Languageâ€‘Model stack**

> Bring documentâ€‘QA, chat, and semantic search to any airâ€‘gapped Windows or Linux networkâ€”no external APIs, no internet needed.

---

## âœ¨ Features

- **Selfâ€‘hosted RAG** â€“ PDF ingestion â†’ Chroma vector store â†’ Crossâ€‘encoder reâ€‘rank â†’ Ollama LLM.
- **Chat + Document QA** â€“ Two endpoints: freeâ€‘form chat or retrievalâ€‘augmented answers.
- **100â€¯% offline reproducible build** â€“ Pinned `requirements.lock`, preâ€‘pulled Ollama weights, Docker images can be exported/imported via `.tar`.
- **FastAPI backend** â€“ ASGIâ€‘native, easy to scale with multiple workers.
- **Modular codebase** â€“ Clear separation: `ingestion.py`, `vector_store.py`, `rerank.py`, `chat.py`, `api.py`.
- **Crossâ€‘platform** â€“ Develop on WindowsÂ 11, deploy on Linux server or WSL2.

---

## ğŸ“‚ Project layout

```text
OfflineLLM/
â”œâ”€ app/
â”‚   â”œâ”€ api.py              # FastAPI routes
â”‚   â”œâ”€ ingestion.py        # PDF loader + splitter
â”‚   â”œâ”€ vector_store.py     # Chroma wrapper
â”‚   â”œâ”€ rerank.py           # Crossâ€‘encoder cache
â”‚   â””â”€ chat.py             # Chat + memory
â”œâ”€ docker/
â”‚   â”œâ”€ Dockerfile
â”‚   â”œâ”€ Ollamafile
â”‚   â”œâ”€ requirements.in
â”‚   â””â”€ entrypoint.sh
â”œâ”€ compose.yaml
â”œâ”€ requirements.lock
â””â”€ docs/
    â””â”€ DEV_SETUP.md
```

---

## ğŸš€ Quickâ€‘start (local dev)

See **docs/DEV_SETUP.md** for the stepâ€‘byâ€‘step guide.  
TL;DR:

```powershell
git clone https://github.com/<yourâ€‘fork>/OfflineLLM.git
cd OfflineLLM
python -m venv .venv
& ".venv\Scripts\Activate.ps1"
python -m pip install --upgrade pip pip-tools
pip-compile docker\requirements.in -o requirements.lock
pip install -r requirements.lock
python -m uvicorn app.api:app --reload
```

Open in browser:

* <http://127.0.0.1:8000/ping>
* <http://127.0.0.1:8000/docs>

---

## ğŸ³ Docker quickâ€‘start

```bash
# build images (oneâ€‘time with internet)
docker compose build

# run the stack
docker compose up -d

# first time only â€“ make sure models are present
docker exec ollama ollama pull llama3:8b-instruct-q3_K_L
```

On startup the backend container runs `python -m app.boot` as the `llm`
user. This indexes any PDFs mounted into `./data/persist` and simply
logs a message if none are found.

### Airâ€‘gap deployment

```bash
# on build machine
docker save -o offline_stack.tar offlinellm-rag-app:latest ollama-offline:latest

# copy to server
docker load -i offline_stack.tar
docker compose up -d
```
## ğŸ”’ Admin mode

Set an `ADMIN_PASSWORD` environment variable in the backend service. When defined, requests to any `/admin/*` endpoint must include the `Authorization: Bearer <ADMIN_PASSWORD>` header.

To permanently ingest a document, use:

```bash
curl -H "Authorization: Bearer $ADMIN_PASSWORD" \
     -F file=@file.pdf \
     http://localhost:8000/admin/upload_pdf
```

When the frontend container is running, open `https://localhost/admin.html` and log in with the same password for a simple upload UI.

---

## ğŸ“š Docs

* **docs/DEV_SETUP.md** â€“ full developer setup
* API usage examples coming soon

---

## ğŸ¤ Contributing

```bash
git checkout -b feature/my-feature
pip install -r requirements.lock
pytest -q
# commit, push, open PR
```

---

## ğŸ“ License

MITÂ â€¦Â TBD before public release.

---

Made with ğŸ’» & â˜• by @hariomahlawat
