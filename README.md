# OfflineLLM

**Offlineâ€‘ready localâ€‘LAN Largeâ€‘Languageâ€‘Model stack**

> Bring documentâ€‘QA, chat, and semantic search to any airâ€‘gapped Windows or Linux networkâ€”no external APIs, no internet needed.

---

## âœ¨ Features

- **Selfâ€‘hosted RAG** â€“ PDF ingestion â†’ Chroma vector store â†’ Crossâ€‘encoder reâ€‘rank â†’ Ollama LLM.
- **Chat + Document QA** â€“ Two endpoints: freeâ€‘form chat or retrievalâ€‘augmented answers.
- **100â€¯% offline reproducible build** â€“ Pinned `requirements.lock`, preâ€‘pulled Ollama weights, Docker images can be exported/imported via `.tar`.
- **FastAPI backend** â€“ ASGIâ€‘native, easy to scale with multiple workers.
- **Modular codebase** â€“ Clear separation: `ingestion.py`, `vector_store.py`, `rerank.py`, `chat.py`, `api.py`.
- **Crossâ€‘platform** â€“ Develop on WindowsÂ 11, deploy on Linux server or WSL2.

---

## ğŸ“‚ Project layout

```text
OfflineLLM/
â”œâ”€ app/
â”‚   â”œâ”€ api.py              # FastAPI routes
â”‚   â”œâ”€ ingestion.py        # PDF loader + splitter
â”‚   â”œâ”€ vector_store.py     # Chroma wrapper
â”‚   â”œâ”€ rerank.py           # Crossâ€‘encoder cache
â”‚   â””â”€ chat.py             # Chat + memory
â”œâ”€ docker/
â”‚   â”œâ”€ Dockerfile
â”‚   â”œâ”€ requirements.in
â”‚   â””â”€ entrypoint.sh
â”œâ”€ compose.yaml
â”œâ”€ requirements.lock
â””â”€ docs/
    â””â”€ DEV_SETUP.md
```

---

## ğŸš€ Quickâ€‘start (local dev)

Follow **docs/DEV_SETUP.md** for the detailed workflow.  
Short version:

```powershell
git clone <yourâ€‘forkâ€‘url> OfflineLLM
cd OfflineLLM
python -m venv .venv
& ".venv\Scripts\Activate.ps1"
python -m pip install --upgrade pip pip-tools
pip-compile docker\requirements.in -o requirements.lock
pip install -r requirements.lock
python -m uvicorn app.api:app --reload
```

Browse to:

* <http://127.0.0.1:8000/ping>
* <http://127.0.0.1:8000/docs>

---

## ğŸ³ Docker quickâ€‘start

```bash
# build images (requires internet once)
docker compose build

# run both containers
docker compose up -d

# first time: seed models
curl http://localhost:11434/api/pull -d '{"name":"llama3:8b"}'
```

To migrate to an airâ€‘gapped server:

```bash
docker save -o offline_stack.tar rag-app:latest ollama-offline:latest
# copy the tar, then
docker load -i offline_stack.tar
docker compose up -d
```

---

## ğŸ“š Documentation

* **DOCS/DEV_SETUP.md** â€“ developer environment  
* **TODO:** Add usage examples and API reference as modules stabilise.

---

## ğŸ¤ Contributing

1. Fork the repo and create your branch: `git checkout -b feature/foo`.
2. Run `pip install -r requirements.lock && pytest -q`.
3. Submit a PR with a clear description of changes.

---

## ğŸ“ License

*Pending â€“ choose MIT or Apacheâ€‘2.0 before first public release.*

---

Made with ğŸ’» & â˜• by the OfflineLLM project.
