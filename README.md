# OfflineLLM

**Offlineâ€‘ready localâ€‘LAN Largeâ€‘Languageâ€‘Model stack**

> Bring documentâ€‘QA, chat, and semantic search to any airâ€‘gapped Windows or Linux networkâ€”no external APIs, no internet needed.

---

## âœ¨ Features

- **Selfâ€‘hosted RAG** â€“ PDF ingestion â†’ Chroma vector store â†’ Crossâ€‘encoder reâ€‘rank â†’ Ollama LLM.
- **Chat + Document QA** â€“ Two endpoints: freeâ€‘form chat or retrievalâ€‘augmented answers.
- **100â€¯% offline reproducible build** â€“ Pinned `requirements.lock`, preâ€‘pulled Ollama weights, Docker images can be exported/imported via `.tar`.
- **FastAPI backend** â€“ ASGIâ€‘native, easy to scale with multiple workers.
- **Modular codebase** â€“ Clear separation: `ingestion.py`, `vector_store.py`, `rerank.py`, `chat.py`, `api.py`.
- **Crossâ€‘platform** â€“ Develop on WindowsÂ 11, deploy on Linux server or WSL2.
- **Model listing** â€“ `/models` enumerates available local LLMs.
- **Sessionâ€‘based uploads + QA** â€“ Upload a PDF via `/upload_pdf`, then query it with `/session_qa`.
- **Dynamic retrieval** â€“ Number of retrieved chunks scales with question length (token based).
- **Offline speech-to-text** â€“ Upload audio to `/speech_to_text` using OpenAI Whisper.

---

## ğŸ“‚ Project layout

```text
OfflineLLM/
â”œâ”€ app/
â”‚   â”œâ”€ api.py              # FastAPI routes
â”‚   â”œâ”€ ingestion.py        # PDF loader + splitter
â”‚   â”œâ”€ vector_store.py     # Chroma wrapper
â”‚   â”œâ”€ rerank.py           # Crossâ€‘encoder cache
â”‚   â””â”€ chat.py             # Chat + memory
â”œâ”€ docker/
â”‚   â”œâ”€ Dockerfile
â”‚   â”œâ”€ Ollamafile
â”‚   â”œâ”€ requirements.in
â”‚   â””â”€ entrypoint.sh
â”œâ”€ compose.yaml
â”œâ”€ requirements.lock
â””â”€ docs/
    â””â”€ DEV_SETUP.md
```

---

## ğŸš€ Quickâ€‘start (local dev)

See **docs/DEV_SETUP.md** for the stepâ€‘byâ€‘step guide.  
TL;DR:

```powershell
git clone https://github.com/<yourâ€‘fork>/OfflineLLM.git
cd OfflineLLM
python -m venv .venv
& ".venv\Scripts\Activate.ps1"
python -m pip install --upgrade pip pip-tools
pip-compile docker\requirements.in -o requirements.lock
pip install -r requirements.lock
winget install ffmpeg   # for speech-to-text
pip install openai-whisper
python -m uvicorn app.api:app --reload
```

Open in browser:

* <http://127.0.0.1:8000/ping>
* <http://127.0.0.1:8000/docs>

---

## ğŸ³ Docker quickâ€‘start

```bash
# build images (oneâ€‘time with internet)
docker compose build

# run the stack
docker compose up -d

# first time only â€“ make sure models are present
docker exec ollama ollama pull llama3:8b-instruct-q3_K_L
```

On startup the backend container runs `python -m app.boot` as the `llm`
user. This indexes any PDFs mounted into `./data/persist` and simply
logs a message if none are found. Ensure this directory exists and is
writable so that admin uploads can be saved.

### Airâ€‘gap deployment

See **docs/OFFLINE_DEPLOY.md** for the full offline workflow. In short,
build/pull the images on a machine with internet access, export them to a
`offline_stack.tar` archive along with the `offline_llm_models/` and `data/`
directories, then load the archive on the server and run `docker compose up -d`.
## ğŸ”’ Admin mode

Set an `ADMIN_PASSWORD` environment variable in the backend service. When defined, requests to any `/admin/*` endpoint must authenticate using HTTP Basic credentials with the `admin` username.

To permanently ingest a document, use:

```bash
curl -u "admin:$ADMIN_PASSWORD" -F file=@file.pdf \
     http://localhost:8000/admin/upload_pdf
```

When the frontend container is running, open `https://localhost/admin.html` and
log in with the same Basic credentials for a simple upload UI. The page now
matches the main site's styling and provides progress feedback along with any
error messages while uploading.

If you serve the backend directly (e.g. `uvicorn` during development) make sure
to build the React frontend with `npm run build` so that the compiled
`admin.html` and assets exist under `offline-llm-ui/dist`. Otherwise visiting
`/admin` will display a blank page. Running the `frontend` container handles the
build automatically.

## ğŸ” Dynamic retrieval depth

Set `RAG_DYNAMIC_K_FACTOR` in the backend service to automatically increase the
number of retrieved chunks for longer questions. The value represents the number
of **tokens** per extra retrieved chunk. For example:

```bash
RAG_DYNAMIC_K_FACTOR=20  # adds one result for every 20 tokens
```

The default is `0`, which disables this behavior entirely.

---

## ğŸ“š Docs

* **docs/DEV_SETUP.md** â€“ full developer setup
* **docs/OFFLINE_DEPLOY.md** â€“ deploy in an offline environment
* API usage examples coming soon

---

## ğŸ¤ Contributing

```bash
git checkout -b feature/my-feature
pip install -r requirements.lock
pytest -q
# commit, push, open PR
```

---

## ğŸ“ License

MIT License â€“ see [LICENSE](LICENSE) for details.

---

Made with ğŸ’» & â˜• by @hariomahlawat
