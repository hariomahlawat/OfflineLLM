
# Developer Setup â€“ **OfflineLLM**

> **Last verified:** 04 Jul 2025  
> **Host OS:** WindowsÂ 11 (PowerShellÂ 7)

---

## 0Â Prerequisites

| Tool | Minimum version | Install notes |
|------|-----------------|---------------|
| **Git** | any recent | <https://git-scm.com/download/win> â€“ add **git.exe** to PATH |
| **Python** | 3.11Â Ã—â€¯64â€‘bit | `python --version` â‡’ *3.11.x* |
| **Docker Desktop** | 4.42â€¯+ | enable **â€œUse the WSLÂ 2 based engineâ€** |
| **Ollama** | 0.3.4 (inside container) | models pulled automatically |
| **VSÂ Code** | optional | Pythonâ€¯+â€¯Docker extensions help |

---

## 1Â Clone & open

```powershell
git clone https://github.com/<yourâ€‘fork>/OfflineLLM.git
cd OfflineLLM
code .         # optional: open folder in VSÂ Code
```

---

## 2Â Local virtualâ€‘env

```powershell
python -m venv .venv
& ".venv\Scripts\Activate.ps1"     # prompt shows (.venv)
python -m pip install --upgrade pip
```

> **Activation blocked?**  
> `Set-ExecutionPolicy -Scope Process -ExecutionPolicy Bypass -Force`

---

## 3Â Dependency locking (pipâ€‘tools)

```powershell
# 1Â â€“ oneâ€‘time install
pip install pip-tools

# 2Â â€“ (re)generate lock file
pip-compile docker\requirements.in -o requirements.lock

# 3Â â€“ install everything
pip install -r requirements.lock
```

For speech-to-text support install **ffmpeg** and the `whisper` Python package:

```powershell
winget install ffmpeg
pip install openai-whisper
```

<details>
<summary>Current <code>docker/requirements.in</code></summary>

```
fastapi
uvicorn[standard]
langchain-community
chromadb==0.5.23
sentence-transformers
pydantic
pymupdf
pypdf
```
</details>

---

## 4Â Run API locally (no Docker)

```powershell
python -m uvicorn app.api:app --reload
```

Browse to:

* <http://127.0.0.1:8000/ping> â†’ `{"status":"ok"}`
* <http://127.0.0.1:8000/docs> â†’ Swagger UI

Stop with **Ctrlâ€‘C**.

---

## 5Â Test PDF ingestion

```powershell
python - <<'PY'
from app.ingestion import load_and_split
chunks = load_and_split("sample.pdf")
print("Chunks:", len(chunks))
print("Preview:", chunks[0].page_content[:200])
PY
```

---

## 6Â Docker workflow (productionâ€‘like)

### 6.1 Build Ollama image (oneâ€‘time)

```powershell
docker build -f docker/Ollamafile -t ollama-offline:latest .
```

Downloads & caches:

* **llama3:8b-instruct-q3_K_L** (~4â€¯GB, fits in 4â€¯GB RAM)
* **nomic-embed-text**

Before bringing up the stack make sure the embedding model is pulled or
ingestion will produce **empty embeddings** and indexing errors:

```powershell
docker exec ollama ollama pull nomic-embed-text
```

### 6.2 BuildÂ / run the RAG API

```powershell
docker compose build rag-app          # fast after first run
docker compose up -d                  # starts ollamaÂ +Â rag-app
```

### 6.3 Health checks

```powershell
Invoke-RestMethod http://localhost:8000/ping          # {"status":"ok"}

Invoke-RestMethod -Method POST `
  -Uri http://localhost:8000/chat `
  -ContentType application/json `
  -Body '{"user_msg":"Hello!"}'
```
### 6.4 React UI (Vite dev server)

```powershell
cd offline-llm-ui
npm install                    # first time only
npm run dev                    # http://localhost:5173/
```

The dev server proxies all `/api` calls to `http://localhost:8000`.
Use `VITE_API_URL=http://localhost:8000` while developing and keep
`VITE_API_URL=/api` for production builds (set in `offline-llm-ui/.env`).

---

## 7Â EnvironmentÂ variables (inside container)

| Variable | Default | Purpose |
|----------|---------|---------|
| `OLLAMA_BASE_URL` | `http://ollama:11434` | internal URL used by ragâ€‘app |
| `OLLAMA_HOST`     | same | fallback for *langchainâ€‘ollama* |
| `CHUNK_SIZE`      | `800` | PDF text-splitter chunk size |
| `CHUNK_OVERLAP`   | `100` | overlap between chunks |
| `RERANK_TOP_K`    | `3` | number of chunks sent to the LLM |
| `RAG_SEARCH_TOP_K` | `10` | how many vectors to retrieve |
| `RAG_USE_MMR`     | `0` | use Max Marginal Relevance retrieval |
| `RAG_DYNAMIC_K_FACTOR` | `0` | tokens per extra retrieved chunk |
| `PERSIST_CHROMA_DIR` | `data/chroma_persist` | permanent embeddings |
| `SESSION_CHROMA_DIR` | `data/chroma_sessions` | per-chat embeddings |
| `ADMIN_PASSWORD` | `None` | protects `/admin/*` endpoints |
| `OLLAMA_DEFAULT_MODEL` | `llama3:8b-instruct-q3_K_L` | default chat model |
| `SYSTEM_PROMPT` | `You are a helpful assistant.` | system prompt sent on first turn |
| `SESSION_TTL_MIN` | `60` | delete idle sessions after *N* minutes |
| `RAG_TOK_LIMIT` | `2000` | truncate history to this many tokens |
| `CORS_ALLOW` | `""` | comma-separated allowed origins |
---

## 8Â Updating dependencies

```powershell
# edit docker/requirements.in
pip-compile docker\requirements.in -o requirements.lock
pip install -r requirements.lock
docker compose build rag-app
```

---

## 9Â Troubleshooting cheatsheet

| Symptom | Fix |
|---------|-----|
| `winget` missing | install Git manually & ensure PATH |
| **400 â€“ model requires more memory** | keep using quantised tag (`q3_K_L`) or set uvicorn workers to **1** |
| *ContainerÂ name conflict* | `docker compose down` or `docker rm -f $(docker ps -aq)` |
| First chat call â‰ˆâ€¯40â€¯s | model loading; subsequent calls â‰ˆâ€¯2â€¯s |

---

*Happy hackingÂ & offlineÂ RAGâ€‘ing!* ğŸš€
